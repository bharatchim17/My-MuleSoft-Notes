--------------------------------------------------------------------------------------
https://kafka.apache.org/documentation/#introduction
--------------------------------------------------------------------------------------
//Kafka: 
Is written in scala & java Apache Kafka is an open-source distributed-streaming & 
messaging system Originally designed by LinkedIn with capabilities to handle huge 
loads of data with its distrubuted , fault tolerence architecture it works with 
pub-sub mechanism for message communication..
database have high storage but low throughput that's why , Operation Per Second is low..
Whereas kafka has high throughput and low storage...

Apache kafka is like communication system that helps different parts of computer system
exchange data by publishing and subscribing to topics..?
//Why apache kafka..?
1. High throughput
2. Fault Tolerance(replication)..

Apache kafka is an event streaming platform What does that mean..?
Kafka combines three key capabilities so we can implement use case for event streaming
end-to-end with single battle-tested solution:
1. To publish(write) and subscribe to (read) streams of events , including continues
import|export of our data from other system..
2. To store streams of events durably and reliably for as long as we want..
3. To process streams of event as they ocure retrospectively(being conscious of what 
happened previously)..

//What is Fault tolerence (replication)..?
Fault Tolerance is defined as the ability of the system to function properly even in 
the presence of any failure..
----------------------------------------------------------------------------------------
 //Kafka architecture:

Kafka Cluster has broker and broker contains topic (topic contains partitions).. 
with help of offset message store in the partition of the topic...

 producer ---> kafka ---> consumer
 kafka ecosystem contains: 1.kafka cluster   2.zookeeper
1. Producer: is an application that sends data to kafka server...
2. Consumer: an application that receive data from kafka sever..
3. Broker: is kafka server..
4. Cluster: group of broker..

5. Topic: is unique name for kafka it contains name related data , they are Link
tables in database , producer produce message into topic (ultimetly to partition in 
round robin fashion)..

6. Partitions:  A topic is split into several parts which are known as the partitions 
of topic , a topic is partitioned and distributed to kafka brokers in round robin 
fashion to achieve distributed system. each partition is an ordered , immutable sequence
of records..

7. Offset: each message gets store into partition with an Incremental id known as 
its offset value..

8. Consumer groups: group of consumers acting as single logic unit..

//What is message..? message is small to medium size piece of data..

E:\Mule-Soft\Message-Broker\kafka-3.7.0-src\bin\windows <---- add this path in 
environment variable...

kafka-3.7.0-src\bin\windows <--- type cmd in this path

//When sending message with key , ordering will be maintained as they will be in the
same partion...
without key we can not garantee the ordering of message as consumer poll the message 
from all the partitions at the same time..

//Consumer offset:
Is position of consumer in specific partition of topic , it represents latest message,
consumer has to read..
When consumer groups reads message from topic , each memeber of the group maintains
its , own offset and updates it as it consumes messages that's  bookmark called 
consumer offset..
Consumer offset is built-in topic in Apache kafka that keeps track of latest offset 
committed for each partition of each consumer group..
-------------------------------------------------------------------------------------

//kafka random commands:

zookeeper-server-start.bat ..\..\config\zookeeper.properties
zookeeper-server-start.bat <----add path of zookeeper.properties

kafka-server-start.bat ..\..\config\server.properties
kafka-server-start.bat <----add path of server.properties

kafka-topics.bat --create --topic my-topic --bootstrap-server localhost:9092 --replication-factor 1 
--partitions 3

kafka-console-producer.bat --broker-list localhost:9092 --topic my-topic

kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic my-topic 
--from-beginning
----------------------------------------------------------------------------------------------------
//Create data folder in path: kafka-3.7.0-src
In data floder create folder: broker & zookeeper
1. zookeeper: 
In config zookeeper.properties dataDir = paste path with forward slash (/) of 
zookeeper folder..

2. broker: 
In config server.properties  log.dirs = paste path with forward slash (/) here of 
broker folder... 

                        ------- or metion directly without creating folder -------

1. In config zookeeper.properties dataDir = E:/Mule-Soft/Message-Broker/kafkaStore/zookeeper

2. In config server.properties  log.dirs= E:/Mule-Soft/Message-Broker/kafkaStore/kafka-logs

-----------------------------------------------------------------------------------------
//kafka Operations:
1. Batch message listener (source)
2. Message listener (source)
3. Consume
4. Commit 
5. Publish
---------------------------------------------------------------------------------
1. Batch message listener: 
This source supports the consumption of messages from a Kafka Cluster, producing a 
List of messages to the flow...
#General:
Poll timeout:
Poll timeout time unit: Days | Hours | Second
Acknowledgement mode: Auto | Dups_Ok | Empty | Immediate | Manual
Amount of parallel consumer: 

2. Message listener:
This source supports the consumption of messages from a Kafka Cluster, producing a 
single message to the flow...

#Global Element
Topic subcription patterns: Edit inline ✅ + | Bean Reference | None
Topic patern:
value: Topic_Name
----------------------------------------------------------------------------------
3. Consume:
This operation allows receiving messages from one or more Kafka topics, it works very 
similarly to the Message Listener source, so all the operations that apply to that, 
will apply to this operation as well..
Note: The consume operation works only in IMMEDIATE mode. The consume operation does 
not return the consumerCommitKey...
#General:
Consumption timeout:
Timeout Time Unit:
Acknowledgement mode:
--------------------------------------------------------------------------------
4. Commit:
Commits the offsets associated to a message or batch of messages consumed in a 
Message Listener. This would be a List or a single message consumed in the Message 
listener or BatchMessageListenerSource ..
#General:
Consumer commit key:
-----------------------------------------------------------------------------------
5. Publish:
Publish a message to the specified kafka topic, optionally specifying the partition, 
key and message content for it. The publish operation supports transactions..
#General:
Topic: Topic_Name
Partion:
Key: #[now()]
Message: #[payload] (default) | "We can put message here"
Headers: 

#Global Element
Bootstrap server: Edit inline ✅ + | Bean Reference
Bootstrap server
Value : localhost:9092
-----------------------------------------------------------------------------------
6. Seek:
Sets the current offset value for the given topic, partition and consumer group of 
the listener..
#General:
Topic:
Partion:
Offset:
------------------------------------------------------------------------------------
//Error Occure:
Classpath is empty. Please build the project first e.g. by running 'gradlew jarAll'
Solution: https://medium.com/@praveenkumarsingh/confluent-kafka-on-windows-how-to-fix-
classpath-is-empty-cf7c31d9c787

//Error Occure:
'wmic' is not recognized as an internal or external command, operable program or batch 
file...

//Error Occure:
Classpath is empty , Please build the project first e.g. by running 'gradlew jarAll'..
Solution: 
---------------------------------------------------------------------------------------
Apache kafka with docker..
E:\Apache kafka\docker-compose.yml
E:\Apache kafka <----- in this path type cmd

//cmd command:
docker-compose -f docker-compose.yml up -d  <----------- for Installation | for running 
docker images
docker ps
docker exec -it acc10e768e66 /bin/sh
ls
cd opt 
ls
cd bitnami
cd kafka <---------- common java kafka licenses scripts
cd bin 

kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --Partitions
 1 --topic dcb 
 -----------------------------------------------------------------------------------
